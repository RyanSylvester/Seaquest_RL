{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578eb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679a1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(frame: np.array):\n",
    "    \"\"\"\n",
    "    Preprocessing\n",
    "    - extract luminosity 0.299*R + 0.587*G + 0.114*B\n",
    "    - reshape to 84x84\n",
    "    \"\"\"\n",
    "    def getLuminosity(r, g, b):\n",
    "        return 0.299 * r + 0.587 * g + 0.114 * b\n",
    "\n",
    "    # reshape\n",
    "    reshaped_frame = tf.image.resize(frame, [84, 84]).numpy()\n",
    "\n",
    "    preprocessed_frame = list()\n",
    "    \n",
    "    for x in reshaped_frame:\n",
    "        x_list = list()\n",
    "        for y in x:\n",
    "            x_list.append(getLuminosity(y[0], y[1], y[2]))\n",
    "        \n",
    "        preprocessed_frame.append(x_list)\n",
    "\n",
    "    return np.array(preprocessed_frame).reshape(84, 84, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768193dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, output_classes):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (8,8), strides=4, activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(64, (4,4), strides=2, activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), strides=1, activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(output_classes)\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da56667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplay():\n",
    "    \"\"\"\n",
    "    Prioritized Replay Memory\n",
    "    - Contains list with replay (state, action reward, next_state)\n",
    "    - Contains list with priorities\n",
    "    - Maximum size of max_size (after this point the instance starts removing the element with the least priority)\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size):\n",
    "        self.replay_memory = list()\n",
    "        self.prio_memory = list()\n",
    "        self.sum_of_prio = 0\n",
    "        self.len = 0\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def add_transition(self, priority, state, action, reward, next_state):\n",
    "        self.replay_memory.append((state, action, reward, next_state))\n",
    "        self.prio_memory.append(priority)\n",
    "        self.len += 1\n",
    "        self.sum_of_prio += priority\n",
    "    \n",
    "    def update_transition_prio(self, index, new_prio):\n",
    "        diff_in_prio = new_prio - self.prio_memory[index]\n",
    "        self.sum_of_prio += diff_in_prio\n",
    "        self.prio_memory[index] = new_prio\n",
    "    \n",
    "    def clean_up(self):\n",
    "        # finds the element that has the lowest prio and removes it\n",
    "        if self.len >= self.max_size:\n",
    "            min_idx = np.argmin(self.prio_memory)\n",
    "            self.replay_memory.pop(min_idx)\n",
    "            self.len -= 1\n",
    "            self.sum_of_prio -= self.prio_memory[min_idx]\n",
    "            self.prio_memory.pop(min_idx)\n",
    "    \n",
    "    def sample(self):\n",
    "        # build probability list\n",
    "        probability_list = [prio/self.sum_of_prio for prio in self.prio_memory]\n",
    "        reference = random.uniform(0, 1)\n",
    "        return_index = -1\n",
    "        return_sample = None\n",
    "        \n",
    "        # return index and sample of the lucky element of the list\n",
    "        current = 0\n",
    "        for i, prob in enumerate(probability_list):\n",
    "            current += prob\n",
    "            \n",
    "            if current < reference:\n",
    "                return_index = i\n",
    "                return_sample = self.replay_memory[i]\n",
    "                \n",
    "                return return_index, return_sample \n",
    "        \n",
    "        return_index = self.len-1\n",
    "        return_sample = self.replay_memory[-1]\n",
    "                \n",
    "        return return_index, return_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15fa48a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "568f8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(environment: gym.Env,\n",
    "          q1_network: tf.keras.models.Sequential, \n",
    "          q2_network: tf.keras.models.Sequential,\n",
    "          network_update_frequency = 10000,\n",
    "          minibatch_size = 32,\n",
    "          min_exploration = 0.1,\n",
    "          max_exploration = 1,\n",
    "          exploration_frame = 1000000,\n",
    "          total_frames=1000000,\n",
    "          replay_memory_size = 5000,\n",
    "          no_op_max = 30,\n",
    "          gamma_discount_factor = 0.99,\n",
    "         learning_rate = 0.00025,\n",
    "         discount_factor = 0.99,\n",
    "         momentum = 0.95,\n",
    "         max_p = 1,\n",
    "         prio_small_pos = 0.05,\n",
    "         training_frequency = 20):\n",
    "    \n",
    "    # Decay for exploratory behavior: starts in exploratory mode and eventually \n",
    "    exploratory_decay_rate = (max_exploration - min_exploration)/exploration_frame\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer=tf.keras.optimizers.RMSprop(\n",
    "        learning_rate=learning_rate,\n",
    "        rho=discount_factor,\n",
    "        momentum=gradient_momentum)\n",
    "    \n",
    "    episode_scores = list()\n",
    "    frame_diff = list()\n",
    "    \n",
    "    frame_number = 0\n",
    "    episode = 0\n",
    "    \n",
    "    # Instance of prioritized replay\n",
    "    replay = PrioritizedReplay(replay_memory_size)\n",
    "    \n",
    "    while frame_number <= total_frames:\n",
    "        state, info = environment.reset()\n",
    "        frame_number = info['frame_number']\n",
    "        \n",
    "        # processed state\n",
    "        proc_state = preprocess(state)\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        replay_memory = list()\n",
    "        n_op = 0\n",
    "\n",
    "        # logging\n",
    "        ep_reward = 0\n",
    "        \n",
    "        max_priority = max_p\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            reference = random.uniform(0, 1)\n",
    "\n",
    "            # exploratory factor\n",
    "            if info['frame_number'] > exploration_frame:\n",
    "                exploratory_factor = min_exploration\n",
    "            else:\n",
    "                exploratory_factor = max_exploration - exploratory_decay_rate * info['frame_number']\n",
    "\n",
    "            # e greedy with linear decay of exploratory factor\n",
    "            if reference < exploratory_factor:\n",
    "                action = environment.action_space.sample() \n",
    "            else:\n",
    "                action = np.argmax(q1_network(np.array([proc_state])))\n",
    "\n",
    "            # take action and observe\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            proc_next_state = preprocess(next_state)\n",
    "\n",
    "            ep_reward += reward\n",
    "\n",
    "            # store in replay memory\n",
    "            replay.add_transition(max_p, \n",
    "                                  np.array([proc_state]), \n",
    "                                  action, \n",
    "                                  reward, \n",
    "                                  np.array([proc_next_state]))\n",
    "            \n",
    "            # keep the replay at a specific size due to constraints on (our) hardware\n",
    "            replay.clean_up()\n",
    "\n",
    "            # increment no op\n",
    "            n_op += 1\n",
    "\n",
    "            # if no_op timeframe is still valid, continue loop\n",
    "            if n_op < no_op_max:\n",
    "                continue\n",
    "            \n",
    "            # train the network every training_frequency operations\n",
    "            if n_op % training_frequency == 0:\n",
    "                accumulated_error = 0\n",
    "\n",
    "                # Gradient Tape records the forward pass\n",
    "                with tf.GradientTape() as tape:\n",
    "                    for _ in range(minibatch_size):\n",
    "                        # Sample replay memory\n",
    "                        index, sample = replay.sample()\n",
    "                        s = sample[0]\n",
    "                        a = sample[1]\n",
    "                        r = sample[2]\n",
    "                        s_= sample[3]\n",
    "\n",
    "                        # TD ERROR\n",
    "                        error = r + gamma_discount_factor*(q2_network(s_)[:, np.argmax(q1_network(s_))]) - q1_network(s)[:, a]\n",
    "                        accumulated_error += error\n",
    "\n",
    "                        replay.update_transition_prio(index, error + prio_small_pos)\n",
    "\n",
    "                # Calculate gradients with respect to every trainable variable\n",
    "                gradients = tape.gradient(accumulated_error, q1_network.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, q1_network.trainable_variables))\n",
    "\n",
    "            # update target network every network_update_frequency operations\n",
    "            if n_op % network_update_frequency == 0:\n",
    "                q2_network = tf.keras.models.clone_model(q1_network)\n",
    "\n",
    "\n",
    "        episode_scores.append(ep_reward)\n",
    "        frame_diff.append(info['frame_number'] - frame_number)\n",
    "\n",
    "        episode += 1\n",
    "        \n",
    "        # saves trained weights every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            action_value_network.save_weights(f'./saved_models/double_dqn/seaquest_action_value_network_ep{episode}')\n",
    "            target_action_value_network.save_weights(f'./saved_models/double_dqn/sequest_target_action_value_network_ep{episode}')\n",
    "\n",
    "            \n",
    "    return episode_scores, frame_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc6aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Seaquest-v5')\n",
    "\n",
    "# vars\n",
    "total_frames = 1000000\n",
    "\n",
    "minibatch_size = 35\n",
    "replay_memory_size = 5000\n",
    "target_network_update_frequency = 100 # corresponds to C in the pseudocode\n",
    "discount_factor = 0.99 # gamma\n",
    "learning_rate = 0.00025\n",
    "gradient_momentum = 0.95\n",
    "initial_exploration = 1\n",
    "final_exploration = 0.1\n",
    "training_frequency = 10\n",
    "\n",
    "final_exploration_frame = total_frames / 50 # 1000000\n",
    "no_op_max = 32\n",
    "\n",
    "action_value_network = create_model((84,84,1), env.action_space.n)\n",
    "\n",
    "target_action_value_network = tf.keras.models.clone_model(action_value_network)\n",
    "\n",
    "scores, frames = train(env,\n",
    "              action_value_network,\n",
    "              target_action_value_network,\n",
    "              no_op_max=no_op_max,\n",
    "              exploration_frame=final_exploration_frame,\n",
    "              network_update_frequency=target_network_update_frequency,\n",
    "              replay_memory_size=replay_memory_size,\n",
    "              total_frames=total_frames,\n",
    "                      training_frequency=training_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ae6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores and episode frames taken from logs\n",
    "\n",
    "scores=[[80, 340, 540, 420, 360],\n",
    "        [240, 520, 360, 300, 320],\n",
    "        [380, 640, 220, 440, 580],\n",
    "        [320, 360, 420, 540, 400],\n",
    "        [380, 420, 360, 260, 220],\n",
    "        [260, 260, 420, 300, 400],\n",
    "        [520, 360, 400, 320, 140],\n",
    "        [380, 240, 180, 620, 440]]\n",
    "\n",
    "frames=[[2189, 4597, 6721, 5554, 5350],\n",
    "        [3934, 6786, 5230, 4330, 5114],\n",
    "        [5606, 8198, 3561, 6125, 7450],\n",
    "        [4985, 4985, 5674, 7033, 6058],\n",
    "        [5242, 6430, 5110, 4089, 3549],\n",
    "        [4790, 4198, 5546, 4713, 6554],\n",
    "        [6774, 5466, 5674, 5098, 2950],\n",
    "        [5098, 3950, 3230, 8094, 5978]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2377e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7ddf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ae90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7ec8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a313025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ce1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44235ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
