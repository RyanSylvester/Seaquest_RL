{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c83514c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 0.1\n",
    "GAMMA = 0.99\n",
    "TARGET_UPDATE = 10\n",
    "REPLAY_CAPACITY = 10000\n",
    "ALPHA = 0.6\n",
    "BETA = 0.4\n",
    "BETA_INCREMENT = 0.001\n",
    "UPDATE_EVERY = 4  \n",
    "BUFFER_SIZE = int(1e5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da34f8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[box2d] in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (0.26.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym[box2d]) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym[box2d]) (1.22.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym[box2d]) (1.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym[box2d]) (4.12.0)\n",
      "Requirement already satisfied: swig==4.* in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym[box2d]) (4.1.1)\n",
      "Collecting box2d-py==2.3.5\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "Requirement already satisfied: pygame==2.1.0 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym[box2d]) (2.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.5.0)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp38-cp38-macosx_10_9_x86_64.whl size=498234 sha256=16be6af9aed10345c81ee93dacebe4ada591889f977bc5ad6141a72084feddf3\n",
      "  Stored in directory: /Users/kamsingh/Library/Caches/pip/wheels/8b/95/16/1dc99ff9a3f316ff245fdb5c9086cd13c35dad630809909075\n",
      "Successfully built box2d-py\n",
      "Installing collected packages: box2d-py\n",
      "Successfully installed box2d-py-2.3.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'gym[box2d]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9fa7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.pos = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        transition = (state, action, reward, next_state, done)\n",
    "        max_priority = self.priorities.max() if self.memory else 1.0\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.pos] = transition\n",
    "        self.priorities[self.pos] = max_priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        if len(self.memory) == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.pos]\n",
    "        probs = priorities ** ALPHA\n",
    "        probs /= probs.sum()\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probs)\n",
    "        experiences = [self.memory[i] for i in indices]\n",
    "        total = len(self.memory)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float()\n",
    "        return (states, actions, rewards, next_states, dones, indices, weights)\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for i, priority in zip(indices, priorities):\n",
    "            self.priorities[i] = priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "778c254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fe19fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = PrioritizedReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "        \n",
    "    def steps(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample(BATCH_SIZE, BETA)\n",
    "                self.learn(experiences, GAMMA)\n",
    "                \n",
    "                \n",
    "#     def act(self, state, eps=0.):\n",
    "#         \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "#         Params\n",
    "#         ======\n",
    "#             state (array_like): current state\n",
    "#             eps (float): epsilon, for epsilon-greedy action selection\n",
    "#         \"\"\"\n",
    "# #         print(state)\n",
    "# #         state_array = np.array(state)\n",
    "# #         print(f'state {state_array} is of type {type(state_array)} and shape {state.shape}')\n",
    "# #         state_array, _ = state\n",
    "#         state_array = state[0]\n",
    "#         state_tensor = torch.from_numpy(state_array).float().unsqueeze(0)\n",
    "        \n",
    "#         state = torch.from_numpy(state_array).float().unsqueeze(0)\n",
    "#         self.qnetwork_local.eval()\n",
    "#         with torch.no_grad():\n",
    "#             action_values = self.qnetwork_local(state)\n",
    "#         self.qnetwork_local.train()\n",
    "\n",
    "#         # Epsilon-greedy action selection\n",
    "#         if random.random() > eps:\n",
    "#             return np.argmax(action_values.cpu().data.numpy())\n",
    "#         else:\n",
    "#             return random.choice(np.arange(self.action_size))\n",
    "\n",
    "## THERE IS SOMETHING WRONG HERE THATS GIVING ME SHAPE PROBLEMS =(\n",
    "    def act(self, state, eps=0.):\n",
    "        # Convert state to tensor\n",
    "        print(f'state position 0 is {state[0:-1]}, and state position 1 is , {state[-1]}')\n",
    "        state_array = state\n",
    "        print(np.array(state[0:-1]))\n",
    "        print(f'state position 0 is {state[0:-1]}, and state position 1 is , {state[-1]}')\n",
    "        \n",
    "        print(len(state))\n",
    "        state_array = np.empty([1,len(state[0:-1])])\n",
    "        state_array[0:-1], _ = state\n",
    "        state_tensor = torch.from_numpy(state_array).float().unsqueeze(0)\n",
    "\n",
    "        # Get action values\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state_tensor)\n",
    "\n",
    "        # Choose epsilon-greedy action\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "            states, actions, rewards, next_states, dones, indices, weights = experiences\n",
    "\n",
    "            # Compute Q targets for next states\n",
    "            Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "            # Compute Q targets for current states\n",
    "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "            # Get expected Q values from local model\n",
    "            Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "            # Compute loss\n",
    "            td_errors = Q_targets - Q_expected\n",
    "            loss = (weights * td_errors ** 2).mean()\n",
    "\n",
    "            # Minimize the loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.memory.update_priorities(indices, td_errors.abs().detach().cpu().numpy())\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update target network\n",
    "            self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "\n",
    "            # Update beta parameter\n",
    "            global BETA\n",
    "            BETA = min(1.0, BETA + BETA_INCREMENT)\n",
    "            \n",
    "            \n",
    "    def soft_update(self, local_model, target_model, tau=0.001):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2693c5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state position 0 is (array([ 1.1356354e-03,  1.3998280e+00,  1.1501759e-01, -4.9298069e-01,\n",
      "       -1.3091781e-03, -2.6053220e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "      dtype=float32),), and state position 1 is , {}\n",
      "2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (8,) into shape (0,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q9/b383m29n0v76jx78r160j92w0000gn/T/ipykernel_59991/1037699127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Select action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'taking action {action}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Take action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q9/b383m29n0v76jx78r160j92w0000gn/T/ipykernel_59991/4184369093.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, eps)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mstate_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mstate_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (8,) into shape (0,1)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# now theres a problem with the environment\n",
    "env = gym.make('ALE/seaquest-v0')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_size=state_size, action_size=action_size, seed=0)\n",
    "\n",
    "n_episodes = 1000\n",
    "max_t = 1000\n",
    "eps_start = 1.0\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.995\n",
    "\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    state = env.reset()\n",
    "    eps = eps_start\n",
    "    for t in range(max_t):\n",
    "        # Select action\n",
    "        action = agent.act(state, eps)\n",
    "        print(f'taking action {action}')\n",
    "        # Take action\n",
    "        next_state, reward, done, info,_ = env.step(action)  # why does this return 5 values not 4???\n",
    "        # Store experience\n",
    "        agent.steps(state, action, reward, next_state, done)\n",
    "        # Update state\n",
    "        state = next_state\n",
    "        # Update epsilon\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        # If episode is done, exit loop\n",
    "        if done:\n",
    "            break\n",
    "    # Print episode score\n",
    "    print(f\"Episode {i_episode} score: {t+1}\")\n",
    "    \n",
    "    # Update beta parameter\n",
    "    agent.memory.beta = min(agent.memory.beta + BETA_INCREMENT, 1)\n",
    "\n",
    "    # Update target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        agent.qnetwork_target.load_state_dict(agent.qnetwork_local.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d7b53b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (0.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym[atari]) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym[atari]) (4.12.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym[atari]) (1.22.4)\n",
      "Collecting ale-py~=0.8.0\n",
      "  Downloading ale_py-0.8.1-cp38-cp38-macosx_10_15_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources\n",
      "  Using cached importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from ale-py~=0.8.0->gym[atari]) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.5.0)\n",
      "Installing collected packages: importlib-resources, ale-py\n",
      "Successfully installed ale-py-0.8.1 importlib-resources-5.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'gym[atari]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e5f29a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (0.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym) (4.12.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from gym) (1.22.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/kamsingh/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2617fb6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment Seaquest doesn't exist. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q9/b383m29n0v76jx78r160j92w0000gn/T/ipykernel_59991/1265757014.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create an instance of the Seaquest environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Seaquest-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0m_check_version_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No registered env with id: {id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0m_check_name_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0msuggestion_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Did you mean: `{suggestion[0]}`?\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuggestion\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     raise error.NameNotFound(\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;34mf\"Environment {name} doesn't exist{namespace_msg}. {suggestion_msg}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     )\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment Seaquest doesn't exist. "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create an instance of the Seaquest environment\n",
    "env = gym.make('Seaquest-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d1d57afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "env_names = [spec.id for spec in gym.envs.registry.values()]\n",
    "print('Seaquest-v0' in env_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9f5b5fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting atari_py\n",
      "  Downloading atari_py-0.2.9-cp37-cp37m-macosx_10_12_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/kamsingh/miniconda3/lib/python3.7/site-packages (from atari_py) (1.21.6)\n",
      "Requirement already satisfied: six in /Users/kamsingh/miniconda3/lib/python3.7/site-packages (from atari_py) (1.16.0)\n",
      "Installing collected packages: atari_py\n",
      "Successfully installed atari_py-0.2.9\n"
     ]
    }
   ],
   "source": [
    "!pip install atari_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9a414ab1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym.envs.atari'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q9/b383m29n0v76jx78r160j92w0000gn/T/ipykernel_59991/810320582.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAtariEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_make_seaquest_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_action_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym.envs.atari'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.envs.atari import AtariEnv\n",
    "from gym import error, spaces\n",
    "\n",
    "def _make_seaquest_env(full_action_space=True):\n",
    "    env = AtariEnv(game='seaquest', obs_type='image', frameskip=1)\n",
    "    if not full_action_space:\n",
    "        env = AtariEnv(game='seaquest', obs_type='image', frameskip=1, full_action_space=False)\n",
    "    return env\n",
    "\n",
    "# Register the Seaquest environment\n",
    "try:\n",
    "    gym.envs.register(\n",
    "        id='Seaquest-v0',\n",
    "        entry_point=_make_seaquest_env,\n",
    "        max_episode_steps=100000,\n",
    "        reward_threshold=250000.0,\n",
    "    )\n",
    "except error.Error:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfd181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
